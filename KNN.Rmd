---
title: "KNN / K-Means"
author: "GWU Intro to Data Science DATS 6101"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r include=FALSE}
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
 
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# KNN  

## Iris dataset  
Let's look at the K Nearest Neighbor, using the FNN package. This is a example from [datacamp](https://www.datacamp.com/community/tutorials/machine-learning-in-r#five) but note that there are some errors in the 'as is' code provided so the below is a more accurate example. 
```{r}
loadPkg("FNN")
#For this example we are going to use the IRIS dataset in R
str(iris)
loadPkg('ggplot2')
```

### Visualization  
Let us start with some basic visualization:  
```{r results='markup'}
#plot(iris[,1],iris[,2], col=iris$Species)
for (xx in 1:(length(iris)-2) ) {
  for (yy in (xx+1):(length(iris)-1) ) {
    # print(xx)
    # print(yy)
    p <- ggplot(iris, aes(x=iris[,xx], y=iris[,yy], color=Species)) +
      geom_point() +
      scale_color_manual(values = c("setosa" = "purple", "versicolor"="orange","virginica"="steelblue")) +
      labs( x = colnames(iris)[xx], y = colnames(iris)[yy], title = paste(colnames(iris)[yy],"vs",colnames(iris)[xx]) )
    print(p)
  }
}
# xx <- 1
# yy <- 2
# ggplot(iris, aes(x=iris[,xx], y=iris[,yy], color=Species)) +
#   geom_point() +
#   scale_color_manual(values = c("setosa" = "purple", "versicolor"="orange","virginica"="steelblue")) +
#   labs( x = colnames(iris)[xx], y = colnames(iris)[yy], title = paste(colnames(iris)[yy],"vs",colnames(iris)[xx]) )
```

### Preparing the data  
First we want to scale and center the data to ensure KNN will operate correctly.  

```{r}
scalediris <- as.data.frame(scale(iris[1:4], center = TRUE, scale = TRUE))
```

We also need to create test and train data sets, we will do this slightly differently by using the sample function. The 2 says create 2 data sets essentially, replacement means we can reset the random sampling across each vector and the probability gives sample the weight of the splits, 2/3 for train, 1/3 for test.  
```{r}
set.seed(1000)
iris_sample <- sample(2, nrow(scalediris), replace=TRUE, prob=c(0.67, 0.33))
```

We then just need to use the new variable to create the test/train outputs, selecting the first four rows as they are the numeric data in the iris data set and we want to predict Species. 

```{r}
iris_training <- scalediris[iris_sample==1, 1:4]
iris_test <- scalediris[iris_sample==2, 1:4]
```

Now we need to create our `Y` variables or labels need to input into the KNN function.  
```{r}
iris.trainLabels <- iris[iris_sample==1, 5]
iris.testLabels <- iris[iris_sample==2, 5]
```

### Build the model  

So now we can deploy our model: 

```{r}
iris_pred <- knn(train = iris_training, test = iris_test, cl=iris.trainLabels, k=7)
iris_pred
```

We can use the `gmodels::CrossTable()` function to check the results of 
the model:  
```{r results='markup'}
loadPkg("gmodels")
IRISPREDCross <- CrossTable(iris.testLabels, iris_pred, prop.chisq = FALSE)
```  
Looks like we got all but three correct. Not too bad, but the dataset is quite small.  

We can also use the `caret::confusionMatrix()` function to further extract the **confusion matrix** info from the results. Let us first try `k=3`:  

```{r}
loadPkg("gmodels")
loadPkg("FNN")
loadPkg("caret") # confusionMatrix

# Loop thru different k values

# create an empty dataframe to store the results from confusion matrices
ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )
```

```{r results='asis'}
kval = 3
iris_pred <- knn(train = iris_training, test = iris_test, cl=iris.trainLabels, k=kval)
IRISPREDCross <- CrossTable(iris.testLabels, iris_pred, prop.chisq = FALSE)
print( paste("k = ", kval) )
IRISPREDCross
# 
cm = confusionMatrix(iris_pred, reference = iris.testLabels ) # from caret library
# print.confusionMatrix(cm)
# 
cmaccu = cm$overall['Accuracy']
print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
# 
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )

```  

Next, we'll loop through the `k` from 4 to 11 and collect the results without printing 
all the details...

```{r}
for (kval in 4:11) {
  iris_pred <- knn(train = iris_training, test = iris_test, cl=iris.trainLabels, k=kval)
  IRISPREDCross <- CrossTable(iris.testLabels, iris_pred, prop.chisq = FALSE)
  print( paste("k = ", kval) )
  IRISPREDCross
  # 
  cm = confusionMatrix(iris_pred, reference = iris.testLabels ) # from caret library
  # print.confusionMatrix(cm)
  # 
  cmaccu = cm$overall['Accuracy']
  print( paste("Total Accuracy = ", cmaccu ) )
  # print("Other metrics : ")
  # print(cm$byClass)
  # 
  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  # cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  ResultDf = rbind(ResultDf, cmt)
  print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
  print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
```  

Finally, let us comparing the accuracy scores for the different `k` values.
```{r, results='markup'}
xkabledply(ResultDf, "Total Accuracy Summary")
```


## Bank dataset   
Next, let us look at a bank customer dataset.  
```{r}
# bank_data = read.csv("bank.csv",                #<- name of the data set.
#                      check.names = FALSE,       #<- don't change column names.
#                      stringsAsFactors = FALSE)  #<- don't convert the numbers and characters to factors.
# colnames(bank_data)[17] = c('signed_up')
bank_data = api_rfit("bank")
# Check the structure and view the data.
str(bank_data)
```  

### Basic info and data prepartion
Here is a brief display of the head/tail of the dataset.  

```{r results='markup'}
xkabledplyhead(bank_data)
xkabledplytail(bank_data)
```  


Before we run the kNN algorithm on our banking data, let us check the composition of labels in the data set.  

```{r results='markup'}
bankSignUpTable <- table(bank_data$signed_up)
bankSignUpTable 
signupRate <- bankSignUpTable[2] / sum(bankSignUpTable)
```


This means that at random, we have a chance of `r 100*signupRate`% correctly picking
out a subscribed individual. Let's see if kNN can do any better.

Let's split the data into a training and a test set.
Sample 80% of our know data as training and 20% as test.

```{r}
set.seed(1)
bank_data_train_rows = sample(1:nrow(bank_data),     #<- from 1 to the number of rows in the data set
                              round(0.8 * nrow(bank_data), 0),  #<- multiply the number of rows by 0.8 and round the decimals
                              replace = FALSE)       #<- don't replace the numbers

# Let's check to make sure we have 80% of the rows. 
length(bank_data_train_rows) / nrow(bank_data)

bank_data_train = bank_data[bank_data_train_rows, ]  #<- select the rows identified in the bank_data_train_rows data
bank_data_test = bank_data[-bank_data_train_rows, ]  #<- select the rows that weren't identified in the bank_data_train_rows data

# Check the number of rows in each set.
nrow(bank_data_train)
nrow(bank_data_test)

```  

### Build the model  

Now, let's train the classifier for k = 3.  We will use the `class` package to run kNN.

```{r}
loadPkg("class")

# k-Nearest Neighbor is a randomized algorithm, so make sure to
# use set.seed() to make your results repeatable.
set.seed(1)
bank_3NN = knn(train = bank_data_train[, c("age", "balance", "duration")],  #<- training set cases
               test = bank_data_test[, c("age", "balance", "duration")],    #<- test set cases
               cl = bank_data_train[, "signed_up"],                         #<- category for true classification
               k = 3) #,                                                    #<- number of neighbors considered
               # use.all = TRUE)                                            #<- control ties between class assignments
                                                                            #   If true, all distances equal to the k-th largest are included

# View the output.
str(bank_3NN)
length(bank_3NN)
table(bank_3NN)
```

How does the kNN classification compare to the `true` class?
Let's take a look at the confusion matrix by combining the predictions from bank_3NN to the original data set.

```{r}
kNN_res = table(bank_3NN, bank_data_test$signed_up)
kNN_res
sum(kNN_res)  #<- the total is all the test examples

# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]

# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc
```
An `r format(kNN_acc*100, digits=3)`% accuracy rate is `r format(kNN_acc/signupRate, digits=3)`x the base rate of `r format(signupRate*100, digits=3)`%, or our chances of classifying people correctly at random.  

We can also offshore these efforts to the library "caret". It requires the test labels to be categorical, however. If you use this `confusionMatrix()` function on a logit model (where the predicted values are probabilities instead of class values), it doesn't work directly. Further processing is needed to compare the probility with a cutoff and then convert prediction to class values. 


```{r}
loadPkg("caret") 
cm = confusionMatrix(bank_3NN, reference = as.factor(bank_data_test[, "signed_up"]) )
cm$overall
cm$byClass
```

### Selecting the correct "k"
How does *k* affect classification accuracy? Let's create a function to calculate classification accuracy based on the number of *k*.
```{r}
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k) #,                #<- number of neighbors considered
                  # use.all = TRUE)       #<- control ties between class assignments. If true, all distances equal to the k-th largest are included
  
  tab = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(tab[row(tab) == col(tab)]) / sum(tab)                         
  cbind(k = k, accuracy = accu)
}

# The sapply() function plugs in several values into our chooseK function.
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop.
knn_different_k = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = bank_data_train[, c("age", "balance", "duration")],
                                             val_set = bank_data_test[, c("age", "balance", "duration")],
                                             train_class = bank_data_train[, "signed_up"],
                                             val_class = bank_data_test[, "signed_up"]))

# Reformat the results to graph the results.
str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg("ggplot2")

ggplot(knn_different_k, aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3) + 
  labs(title = "accuracy vs k")

```  
 
It seems 5-nearest neighbors is a decent choice because that's the greatest improvement in predictive accuracy before the incremental improvement trails off.  




# K-means:
## Toy example  

Let us create a simple 1-var "dataset".  

```{r}
v1=c(2,3,4,10,11,12,20,25,30) # use the example in the pptx, try k=2 clusters
for( cut in 2:(length(v1)-2) ) {
    print(
        paste(
            "mean 1 = ",mean(v1[1:cut]), 
            "mean 2 = ", mean(v1[(cut+1):length(v1)]),
            "Sum of Sqaures  = ", (cut-1)*var(v1[1:cut]) + (length(v1)-cut-1)*var(v1[(cut+1):length(v1)]),
            "cluster 1 len = ", cut, "cluster 2 len = ", (length(v1) - cut)
        )
    ) 
}

```  

We minimize the total sum of squares within clusters, and we found the split is 6,3. 

We can also simply use the function kmeans in R like this:  
```{r}
v1clusters <- kmeans(v1,3)
v1clusters
```

## Toy example - 2D  
Now, let us modifer the dataset to have a second variable.  

```{r}
v2 <- NULL
v2 <- as.data.frame(v1)
colnames(v2) <- c("var1")
v2$var2 <- c(30,21,26,15,8,11,5,9,6)
plot(v2[,1:2], main = "K-means, var2 vs var1")
v2
```

We will then try to find 3 clusters:  
```{r results='markup'}
v2clusters3 <- kmeans(v2,3)
v2clusters3
```  

We can also add the labels to the dataframe:  
```{r}
v2$label = v2clusters3$cluster
xkabledplyhead(v2, rows = 2)
xkabledplytail(v2, rows = 2)
plot(v2$var1,v2$var2, col=v2$label, main = "3 clusters: var2 vs var1", xlab = "var1", ylab = "var2")
ggplot(v2, aes(x=var1, y=var2, color= factor(label) )) + geom_point() + labs( title = "3 clusters: var2 vs var1", color="Cluster" )
```

And now try to force it into 2 clusters:  
```{r}
v2clusters2 <- kmeans(v2,2)
#v2clusters2
v2$label = v2clusters2$cluster
xkabledplyhead(v2, rows = 2)
xkabledplytail(v2, rows = 2)
plot(v2$var1,v2$var2, col=v2$label, main = "2 clusters: var2 vs var1", xlab = "var1", ylab = "var2")
ggplot(v2, aes(x=var1, y=var2, color= factor(label) )) + geom_point() + labs( title = "2 clusters: var2 vs var1", color="Cluster" )
```

